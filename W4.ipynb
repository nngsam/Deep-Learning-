{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM1KN2DqqTkSakTcS2lvfAL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nngsam/Deep-Learning-/blob/main/W4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10, MNIST, FashionMNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n"
      ],
      "metadata": {
        "id": "QgzHVU3m1Jck"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0nFjt4yoEHN",
        "outputId": "8334a4d0-ce8b-4470-ea3f-3c9f75f28d36"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data augmentation"
      ],
      "metadata": {
        "id": "6K1w8uzA05Ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize và thêm các transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
        "    transforms.RandomResizedCrop(size=28, scale=(0.8, 1.0)),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomHorizontalFlip()\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "aXfpMz2XfEcM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the FashionMNIST dataset\n",
        "batch_size = 64\n",
        "train_dataset = FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "WS_Tl3OnfP9Q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "JiQXCc4yfd4Y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defnied VGG provided\n",
        "class MiniVGG(nn.Module):\n",
        "    def __init__(self, num_classes= 10):\n",
        "        super(MiniVGG, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels= 1, out_channels= 64, kernel_size= (3,3), stride= (1,1), padding= 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels= 64, out_channels= 64, kernel_size= (3,3), stride=(1,1), padding= 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size= (2,2), stride= (2,2)),\n",
        "\n",
        "            nn.Conv2d(in_channels= 64, out_channels= 128, kernel_size= (3,3), stride= (1,1), padding= 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels= 128, out_channels= 128, kernel_size= (3,3), stride=(1,1), padding= 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size= (2,2), stride= (2,2)),\n",
        "\n",
        "            nn.Conv2d(in_channels= 128, out_channels= 256, kernel_size= (3,3), stride= (1,1), padding= 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels= 256, out_channels= 256, kernel_size= (3,3), stride=(1,1), padding= 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size= (2,2), stride= (2,2)))\n",
        "\n",
        "        self.classifier = nn.Linear(256 * 3 * 3, 10)\n",
        "        nn.init.normal_(self.classifier.weight, 0, 0.01)\n",
        "        nn.init.constant_(self.classifier.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "wrCrugHV1Ekg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fintuning"
      ],
      "metadata": {
        "id": "g-TGAstpgK7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained models on CIFAR-10 and MNIST\n",
        "cifar10_model = MiniVGG().to(device)\n",
        "cifar10_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/cifar10_mini_vgg.pth\", map_location=torch.device('cpu')))\n",
        "for param in cifar10_model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in cifar10_model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "mnist_model = MiniVGG().to(device)\n",
        "mnist_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/mnist_mini_vgg.pth\", map_location=torch.device('cpu')))\n",
        "for param in mnist_model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in mnist_model.classifier.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "je4HkDM404Ek"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scratch_model = MiniVGG().to(device)"
      ],
      "metadata": {
        "id": "p3Bm_nr6oDEn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scratch_model"
      ],
      "metadata": {
        "id": "gPyol4Rf04Jg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c7916e-595d-4010-afb3-57f07df902f0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MiniVGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU()\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU()\n",
              "    (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU()\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU()\n",
              "    (14): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Linear(in_features=2304, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10_model"
      ],
      "metadata": {
        "id": "5f7OqVot04MG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a718c26-3dbf-4a5e-fdbe-0de45ff1361a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MiniVGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU()\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU()\n",
              "    (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU()\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU()\n",
              "    (14): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Linear(in_features=2304, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_model"
      ],
      "metadata": {
        "id": "kJMeqNiV04N-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0750c138-1c5c-4890-e44a-7834b5e11dd0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MiniVGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU()\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU()\n",
              "    (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU()\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU()\n",
              "    (14): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Linear(in_features=2304, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cWGSwF1V04S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(scratch_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Set the batch size of the model\n",
        "#model.batch_size = 3\n",
        "\n",
        "for epoch in range(5):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = scratch_model(images)\n",
        "        #loss.requires_grad = True\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        #loss.requires_grad = True\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(\"Epoch: {}/5, Step: {}/{}, Loss: {:.4f}\".format(\n",
        "                epoch + 1, i + 1, len(train_loader), loss.item()\n",
        "            ))\n",
        "\n",
        "scratch_model.eval()\n",
        "with torch.no_grad():\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch_size = images.size(0)\n",
        "        outputs = scratch_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "\n",
        "    print(f'Learning Rate: 0.001, Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "kMWXGxat04Wk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c001b2-d2fb-43d1-cdf5-e5391a3768f5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5, Step: 100/938, Loss: 2.3014\n",
            "Epoch: 1/5, Step: 200/938, Loss: 2.2977\n",
            "Epoch: 1/5, Step: 300/938, Loss: 2.2802\n",
            "Epoch: 1/5, Step: 400/938, Loss: 2.2377\n",
            "Epoch: 1/5, Step: 500/938, Loss: 2.1053\n",
            "Epoch: 1/5, Step: 600/938, Loss: 1.5728\n",
            "Epoch: 1/5, Step: 700/938, Loss: 1.0841\n",
            "Epoch: 1/5, Step: 800/938, Loss: 0.9977\n",
            "Epoch: 1/5, Step: 900/938, Loss: 0.9802\n",
            "Epoch: 2/5, Step: 100/938, Loss: 0.8069\n",
            "Epoch: 2/5, Step: 200/938, Loss: 0.7696\n",
            "Epoch: 2/5, Step: 300/938, Loss: 0.9956\n",
            "Epoch: 2/5, Step: 400/938, Loss: 0.7657\n",
            "Epoch: 2/5, Step: 500/938, Loss: 0.7863\n",
            "Epoch: 2/5, Step: 600/938, Loss: 0.8890\n",
            "Epoch: 2/5, Step: 700/938, Loss: 0.6647\n",
            "Epoch: 2/5, Step: 800/938, Loss: 0.6409\n",
            "Epoch: 2/5, Step: 900/938, Loss: 0.5834\n",
            "Epoch: 3/5, Step: 100/938, Loss: 0.8001\n",
            "Epoch: 3/5, Step: 200/938, Loss: 0.7006\n",
            "Epoch: 3/5, Step: 300/938, Loss: 0.8928\n",
            "Epoch: 3/5, Step: 400/938, Loss: 0.7827\n",
            "Epoch: 3/5, Step: 500/938, Loss: 0.7259\n",
            "Epoch: 3/5, Step: 600/938, Loss: 0.9950\n",
            "Epoch: 3/5, Step: 700/938, Loss: 0.8946\n",
            "Epoch: 3/5, Step: 800/938, Loss: 0.7402\n",
            "Epoch: 3/5, Step: 900/938, Loss: 0.6771\n",
            "Epoch: 4/5, Step: 100/938, Loss: 0.5788\n",
            "Epoch: 4/5, Step: 200/938, Loss: 0.6466\n",
            "Epoch: 4/5, Step: 300/938, Loss: 0.6065\n",
            "Epoch: 4/5, Step: 400/938, Loss: 0.6003\n",
            "Epoch: 4/5, Step: 500/938, Loss: 0.4521\n",
            "Epoch: 4/5, Step: 600/938, Loss: 0.4867\n",
            "Epoch: 4/5, Step: 700/938, Loss: 0.5252\n",
            "Epoch: 4/5, Step: 800/938, Loss: 0.4257\n",
            "Epoch: 4/5, Step: 900/938, Loss: 0.6137\n",
            "Epoch: 5/5, Step: 100/938, Loss: 0.4841\n",
            "Epoch: 5/5, Step: 200/938, Loss: 0.4417\n",
            "Epoch: 5/5, Step: 300/938, Loss: 0.5453\n",
            "Epoch: 5/5, Step: 400/938, Loss: 0.5959\n",
            "Epoch: 5/5, Step: 500/938, Loss: 0.5702\n",
            "Epoch: 5/5, Step: 600/938, Loss: 0.4934\n",
            "Epoch: 5/5, Step: 700/938, Loss: 0.5751\n",
            "Epoch: 5/5, Step: 800/938, Loss: 0.7106\n",
            "Epoch: 5/5, Step: 900/938, Loss: 0.4560\n",
            "Learning Rate: 0.001, Accuracy: 0.7805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(cifar10_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Set the batch size of the model\n",
        "#model.batch_size = 3\n",
        "\n",
        "for epoch in range(5):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = cifar10_model(images)\n",
        "        #loss.requires_grad = True\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        #loss.requires_grad = True\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(\"Epoch: {}/5, Step: {}/{}, Loss: {:.4f}\".format(\n",
        "                epoch + 1, i + 1, len(train_loader), loss.item()\n",
        "            ))\n",
        "\n",
        "cifar10_model.eval()\n",
        "with torch.no_grad():\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch_size = images.size(0)\n",
        "        outputs = cifar10_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "\n",
        "    print(f'Learning Rate: 0.001, Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "jDdfPAFOrJvu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(mnist_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Set the batch size of the model\n",
        "#model.batch_size = 3\n",
        "\n",
        "for epoch in range(5):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = mnist_model(images)\n",
        "        #loss.requires_grad = True\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        #loss.requires_grad = True\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(\"Epoch: {}/5, Step: {}/{}, Loss: {:.4f}\".format(\n",
        "                epoch + 1, i + 1, len(train_loader), loss.item()\n",
        "            ))\n",
        "\n",
        "mnist_model.eval()\n",
        "with torch.no_grad():\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch_size = images.size(0)\n",
        "        outputs = mnist_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    accuracy = total_correct / total_samples\n",
        "\n",
        "    print(f'Learning Rate: 0.001, Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "bBdmjwAyrcBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VV1DwK0SrcE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model on FashionMNIST\n",
        "# Test the models\n",
        "def test_model(model, name):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of {name}: {accuracy:.2f}%')\n",
        "\n",
        "test_model(cifar10_model, \"CIFAR-10 Model\")\n",
        "test_model(mnist_model, \"MNIST Model\")\n",
        "test_model(scratch_model, \"Scratch Model\")"
      ],
      "metadata": {
        "id": "H2fMsB_K04vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Feature Extraction"
      ],
      "metadata": {
        "id": "nmWBWVOLp5WY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.feature_extraction import get_graph_node_names\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "\n",
        "train_nodes, eval_nodes = get_graph_node_names(scratch_model)"
      ],
      "metadata": {
        "id": "YaFDywFTp_ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_nodes"
      ],
      "metadata": {
        "id": "l4_Bn8u3p_wP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_feature_extractor(scratch_model, train_return_nodes= train_nodes, eval_return_nodes= eval_nodes)"
      ],
      "metadata": {
        "id": "GZkSbCx_p_yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scratch_model.features[0].weight"
      ],
      "metadata": {
        "id": "lbORfk1EqPHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pxghGx-SqPK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aEojiGrOsk2m"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uxMit3mvsk7v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}